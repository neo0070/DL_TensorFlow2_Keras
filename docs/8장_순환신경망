# 08. 순환 신경망

#### RNN(Recurrent Neural Networks)이란?
- 텍스트 입력에 널리 사용되는 신경망 클래스
- 매우 유연하고 음성인식, 언어 모델링, 기계 번역, 감정 분석, 이미지 캡션관 같은 문제를 해결하는데 사용
- 입력이 순차적이라는 특성을 이용 - 텍스트, 음성, 시계열 등

<br><br>

## 기본 RNN 셀
전통적인 다층 퍼셉트론 신경망은 모든 입력이 서로 독립적이라 가정하지만, 여러유형의 시퀀스 데이터(문장의 단어, 시간에 따른 주가)에는 해당하지 않음  
RNN 셀을 통한 데이터의 흐름, 즉 입력과 은닉 상태를 결합해 다음 상태를 생성하여 재귀적으로 시퀀스 데이터를 처리
> 그림 8-1 참조

- 입력과 상태의 결합 : 두가지 입력값 - 현재 시간 단계의 입력 데이터(U), 이전 시간 단계의 상태(h_t-1)
- 상태의 업데이트 : 두 입력과 은닉 상태(W)를 통해 셀의 현재 상태를 계산 
- 출력의 생성 

$$
h_t = \text{tanh}(Wh_{t-1} + Ux_t)
$$

<br>

### BPTT
시간 흐름에 따른 그래디언트 역전파, 가중치가 모든 타임 스텝에 의해 공유

- 시간에 따른 전개 : RNN을 시간에 따라 전개하여, 각 시간 단계에서의 네트워크를 별도의 레이어로 취급
- 오차의 계산 : 출력층에서의 오차는 실제 값과 예측 값 사이의 차이로 계산, 오차는 시간을 역행해 각 레이어의 가중치에 대한 그라디언트를 계산하는 데 사용
- 그라디언트의 역전파 : 계산된 그라디언트는 시간을 역행해 각 레이어의 가중치를 조정하는 데 사용, 각 시간 단계의 가중치 업데이트는 이전 단계의 그라디언트에 영향
- 가중치 업데이트: 모든 시간 단계에 대한 그라디언트가 계산되면, 이를 사용하여 네트워크의 가중치를 업데이트


<br>

### 사라지고 폭발하는 그래디언트
긴 시퀀스를 처리할 때 그라디언트 소실 또는 폭발 문제 발생 가능성  
그라디언트가 네트워크를 거슬러 올라갈 때 점차 감소하거나 증가하는 현상으로, 학습 과정을 불안정하게 함  
> 그라디언트가 1보다 작으면 곱은 점점 작아져 소멸되고 1보타 크면 곱이 점점 커지면서 폭발

이를 극복하기 위해 LSTM이나 GRU 사용

<br><br>

## RNN 셀 변형
### LSTM
> Long short-term memory

그라디언트 소실 문제를 효과적으로 해결하여, 긴 시퀀스 데이터에서 장기 의존성을 학습  
다양한 종류의 시퀀스 데이터(예: 텍스트, 시계열 데이터)에 적용할 수 있으며, 다양한 문제(예: 분류, 예측, 생성)에 사용  

셀 상태(cell state)라는 내부 메커니즘를 사용하여 정보를 장기간 유지, 셀 상태를 조절하기 위해 여러 게이트를 사용  
- 망각 게이트 (Forget Gate): 셀 상태에서 어떤 정보를 버릴지 결정, 현재 입력과 이전 시간 단계의 출력을 받아, 0과 1 사이의 값을 출력
- 입력 게이트 (Input Gate): 새로운 정보를 셀 상태에 얼마나 추가할지 결정하고 실제로 추가할 정보를 생성
- 셀 상태 업데이트: 망각 게이트와 입력 게이트의 출력을 사용하여 셀 상태를 업데이트
- 출력 게이트 (Output Gate): 다음 시간 단계로 넘겨줄 출력을 결정, 현재 입력과 이전 출력을 고려결정

<br>

### GRU
> Grated Recurrent Unit

LSTM과 유사한 목적으로 설계되었지만, 구조가 더 단순하여 은닉상태의 갱신을 위한 계산량이 더 적어 훈련이 더 빠름

- 업데이트 게이트 (Update Gate): 셀의 현재 입력과 이전 상태를 고려하여 이전 상태를 얼마나 유지할지 결정
- 재설정 게이트 (Reset Gate): 이전 상태를 얼마나 버릴지 결정하며 새로운 입력과 결합

<br>

### 핍홀 LSTM 
> Peephole LSTTM

셀 상태에 대한 핍홀 연결을 추가하여 게이트가 셀 상태의 일부 정보를 엿볼 수 있는 엿보기 구멍을 추가하여, 게이트의 결정을 도움

- 핍홀 연결: 핍홀 LSTM에서는 망각 게이트(Forget Gate), 입력 게이트(Input Gate), 그리고 출력 게이트(Output Gate) 각각이 셀 상태에 직접 연결
- 게이트의 개선된 결정: 게이트가 셀 상태 의 정보도 고려하여 결정
  - 얼마나 많은 정보를 잊어야 하는지 
  - 새로운 정보를 얼마나 추가할지
  - 출력을 얼마나 조절할지

<br><br>

## RNN 변형
특정 상황에서 성능향상을 제공할 수 있는 기본 RNN 아키텍처의 몇가지 변형 살펴보기  
이러한 전략은 다른 종류의 RNN 셀과 다른 RNN 위상에도 적용 가능  

<br>

### 양방향 RNN
시퀀스 데이터를 양방향 -과거와 미래로 처리할 수 있도록 설계  
두 개의 별도의 RNN 레이어를 사용 - 정방향 레이어, 역방향 레이어
- 시퀀스 처리: 시퀀스의 각 요소는 두 개의 RNN 레이어(정방향 및 역방향)에 동시에 입력
- 출력 결합: 각 시간 단계에서 두 레이어의 출력은 결합되어 최종 출력
- 정보의 통합: 이 구조를 통해 모델은 각 시간 단계에서 과거와 미래의 정보를 모두 활용

<br>

### 상태 저장 RNN 
연속된 학습 배치 간에 내부 상태(즉, 메모리)를 유지  
배치간의 내부 상태가 유지되어 시퀀스 데이터의 장기적인 의존성을 더 잘 학습할수 있도록 함
> 일반적인 RNN이나 LSTM은 각 학습 배치를 독립적으로 처리하고 배치 간에 내부 상태를 초기화 

- 상태 유지: 이전 배치의 마지막 상태를 다음 배치의 초기 상태로 사용
- 장기 의존성 학습: 긴 시퀀스 데이터에서 장기 의존성을 학습하는 데 유용, 상태 정보가 유지되기 때문에 네트워크는 더 긴 시간 범위에 걸친 패턴과 관계를 학습
- 배치 처리의 중요성: 각 배치의 시퀀스 길이는 일정해야 하며, 연속된 배치들이 시퀀스 데이터에서 시간적으로 연결되어 있어야 함

<br><br>

## RNN 위상
RNN의 구조나 상태를 시각적으로 표현하는 방법을 의미하며 네트워크의 구조적 특성과 시간에 따른 동작을 이해하는 데 도움  
RNN은 시퀀스 입력과 출력이 가능 - RNN 셀을 다른방식으로 배열해 다른 유형의 문제를 해결


<br>

### 예제: 일대다 텍스트 생성 학습
언어모델로 어휘에서 다른 단어의 출력 확률을 샘플링해 텍스트를 생성할 수 있게 되는 생성 모델   
루이스 캐롤의 "이상한 나라의 앨리스", "거울 나라의 앨리스"의 텍스트 기반을 둔 문자 기반 RNN 훈련  
작은 어휘로도 더 빨리 훈련할 수 있는 특성 떄문에 캐릭터 기반 모델 구축   
> 단어 대신 문자를 사용하는 점을 제외하면 단어기반 언어 모델 훈련과 동일

- 초기화: 생성 과정은 보통 특정 시작 토큰(예: 시작 단어나 문자)으로 시작하며, 이 토큰은 RNN의 초기 입력으로 사용됩니다.
- 순환적 예측: RNN은 이 입력을 받아 내부 상태를 업데이트하고, 다음 토큰(예: 다음 단어나 문자)을 예측하고, 생성된 토큰은 다음 입력으로 사용
- 시퀀스 생성: RNN이 종료 토큰을 생성하거나 미리 정해진 길이에 도달할 때까지 반복되고, 각 단계에서 이전 단계의 출력과 현재의 내부 상태를 기반으로 다음 토큰을 생성

<br>

### 예제: 다대일 감정 분석
다대일 신경망을 사용해 문장을 입력으로 받은 후 해당 감정을 양수(긍정)나 음수(부정)로 예측하여 레이블

- 시퀀스 입력: 문장 또는 문서와 같은 텍스트 시퀀스를 입력으로 받으며, 이 시퀀스는  토큰화되어 개별 단어나 문자로 분리
- 순환 처리: 시퀀스의 각 요소(예: 단어)를 순차적으로 처리하며, 각 단계에서 내부 상태를 업데이트, 이 상태는 시퀀스의 현재까지의 정보를 요약
- 최종 상태: 시퀀스의 마지막 요소를 처리한 후 최종 상태는 전체 시퀀스에 대한 정보를 포함, 이 상태는 시퀀스 전체의 의미를 요약하는 데 사용
- 출력 레이어: RNN의 최종 상태는 하나 이상의 완전 연결 레이어(Dense Layer)를 통과하여 최종 출력을 생성합, 감정 분석에서는 이 출력이 감정 레이블(예: 긍정, 부정)로 변환

<br>

### 예제: 다대다 POS 태깅
GRU 계층을 사용해 POS(품사 - 명사, 동사, 형용사 등) 태깅을 수행하는 신경망 구축

- 시퀀스 입력: 문장이나 문구가 입력되며 문장은 토큰화되어 개별 단어로 분리
- 순환 처리: 문장의 각 단어를 순차적으로 처리, 각 단계에서 RNN은 내부 상태를 업데이트하며 이 상태는 이전 단어들의 정보를 포함
- 시퀀스 출력: 입력 시퀀스의 각 단어에 대해 해당하는 품사 태그를 출력, 입력 시퀀스와 동일한 길이의 출력 시퀀스가 생성
- 출력 레이어: 각 단계에서 출력은 품사 태그에 대한 예측으로 변환, 소프트맥스 레이어를 통해 각 태그에 대한 확률 분포로 변환

<br><br>

## 인코더-디코더 아키텍처: seq2seq
RNN 기반의 인코더와 디코더로 구성, 마자막의 단일 결합 출력대신 여러 타임 스텝에 해당하는 출력 시퀀스를 소비하고 반환할 수 있음  
> 신경망의 출력을 생성하고자 모든 입력이 소비될 떄까지 기다릴 필요가 없음

- 인코더: 입력 시퀀스(예: 문장)를 받아들여 고정된 크기의 컨텍스트 벡터(context vector)로 변환
- 디코더: 인코더로부터 받은 컨텍스트 벡터를 사용하여 출력 시퀀스(예: 번역된 문장)를 생성, 각 시간 단계에서 다음 출력 토큰을 예측
- 학습 단계: 인코더에 입력 시퀀스를 제공하고, 디코더는 해당 시퀀스에 대응하는 목표 시퀀스를 생성하도록 학습, 디코더는 이전 시간 단계의 출력을 다음 시간 단계의 입력으로 사용
- 추론 단계: 모델을 사용할 때 인코더는 입력 시퀀스를 컨텍스트 벡터로 변환하고, 디코더는 이 벡터를 사용하여 전체 출력 시퀀스를 한 번에 하나씩 생성

<br>

가장 큰 적용 분야는 신경 기계 번역이였지만 번역구조로 구조화 될 수 있는 문제(문자 파싱, 이미지 캡션)에도 적용

<br><br>

## 예제: 기계 번역 용 어텐션 없는 seq2seq
프랑스어-영어의 이중 언어 데이터셋을 사용해 영어를 프랑스어로 번역하는 방법을 학습하는 예제

<br><br>

## 어텐션 메카니즘
디코더의 모든 타임 스텝에서 모든 인코더 은닉 상태에 대한 액세스를 제공  
입력 시퀀스의 특정 부분에 주의를 집중하도록 함으로써 긴 시퀀스를 처리할 때 성능을 크게 향상  
어텐션 가중치를 분석함으로써, 모델이 어떤 입력 부분에 주목했는지 이해하고 모델의 결정을 해석하는 데 도움이 됨  

<br>

### 예제: 기계 번역 용 어텐션이 있는 seq2seq
디코더가 생성할 각 단어에 대해 입력 시퀀스의 어떤 부분에 집중해야 할지 결정, 모델이 입력 시퀀스의 모든 부분을 동일하게 처리하는 대신 현재 생성하려는 단어와 가장 관련이 높은 부분에 주의하도록 함

- 인코더: 입력 시퀀스(예: 원문)를 처리하고, 각 단어/토큰에 대한 표현(인코딩된 벡터)을 생성
  - 어텐션 메카니즘에서 사용할 수 있도록 매 시간 포인트마다 출력이 반환
- 디코더: 인코더의 출력과 자체 내부 상태를 기반으로 출력 시퀀스(예: 번역문)를 한 단어씩 생성
  - 각 타임 스텝에서 디코더 은닉상태를 모든 인코더 은닉상태와 결합해 디코더의 입력을 생성 - 현재 단어를 생성하는 데 가장 유용한 부분을 결정  
- 어텐션 메커니즘: 인코더의 각 단어에 대한 가중치를 계산하고, 이를 사용하여 컨텍스트 벡터를 생성  
  - 디코더의 상태와 결합되어 다음 단어를 예측하는 데 사용됩니다.  


<br><br>
## 변환기 아키텍처
인코더 디코더의 아이디어와 어텐션을 결합한 심층 신경망, 변환기 신경망을 사용해 텍스트 처리
반복계층이 어텐션 계층으로 대체된 인코더-디코더 아키텍처의 변형
> seqseq와의 차이점 참조 : 그림 8-7

- 인코더와 디코더 블록: 각 인코더 블록은 입력 시퀀스를 처리하고, 각 디코더 블록은 출력 시퀀스를 생성
- 셀프 어텐션(Self-Attention): 한 시퀀스 내의 각 위치가 시퀀스 내 다른 위치와 어떻게 관련되어 있는지를 파악, 각 단어의 문맥을 더 잘 이해
- 포지션 와이즈 피드-포워드 네트워크(Position-wise Feed-Forward Networks):
  - 각 인코더와 디코더 블록에는 피드-포워드 신경망이 포함되어 있으며, 이는 각 위치에서의 표현을 독립적으로 변환
- 포지셔널 인코딩(Positional Encoding): 순환 또는 컨볼루션 레이어를 사용하지 않기 때문에, 시퀀스의 순서 정보를 모델에 제공하기 위해 포지셔널 인코딩을 사용

<br>

#### 특징
- 병렬 처리:데이터의 시퀀스를 한 번에 처리할 수 있어, 훈련과 추론에서 높은 병렬 처리 능력을 가짐
- 장거리 의존성 학습: 셀프 어텐션 메커니즘 덕분에, 긴 시퀀스에서도 장거리 의존성을 효과적으로 학습
- 유연성: 기계 번역, 텍스트 요약, 질문 응답 시스템 등 다양한 자연어 처리 작업에 적용





































